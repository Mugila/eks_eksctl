apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: eks-poc
  region: us-east-1
  version: "1.30"
  tags:
    wp:managed-by: "eksctl"
    cluster: "eks-poc"
    owner: "ops"
    purpose: dev  # Purpose of the cluster (e.g., dev, test, production)
    scope: poc # Scope or boundary of usage (e.g., team-specific, project-specific)
    version: "1.30" # This is the latest Kubernetes version currently supported by Harness SMP
    karpenter.sh/discovery: eks-poc  #${CLUSTER_NAME}

vpc:
  id: vpc-0f941eed71c3b0179
 # publicAccessCIDRs:  ["10.0.0.0/16"] # Allow access from anywhere (for demonstration, restrict in production)
  #autoAllocateIPv6: false
 # manageSharedNodeSecurityGroupRules: false
  #nat:
   # gateway: Disable
  #publicAccessCIDRs: ["10.0.2.0/24", "10.0.9.0/25"]
  securityGroup: "sg-06a0beefd0ed45a07"
  subnets:
    private:
     us-east-1a: { id: subnet-0ebeafd6054e83d84 }
     us-east-1b: { id: subnet-0e62b2a66ea4235c7 }
     # us-east-1c: { id: subnet-0564dca4816439969 }
    public:
      us-east-1a: { id: subnet-0441e1c0571e6d4a6 }
      us-east-1b: { id: subnet-0b352436511bed6c4 }
      #us-east-1c: { id: subnet-02ab36d3b3700e7fa }
  clusterEndpoints:
    privateAccess: false # True explicitly enables the private endpoint for the EKS API server within your VPC
    publicAccess: true # True explicitly enables the public endpoint for the EKS API server.
    
cloudWatch:
   clusterLogging:
    enableTypes: ["api", "audit", "authenticator", "controllerManager", "scheduler"]
    logRetentionInDays: 7
iam:
 vpcResourceControllerPolicy: true
 withOIDC: true
 podIdentityAssociations: 
 #serviceAccounts:	
  - namespace: kube-system
    serviceAccountName: aws-load-balancer-controller
    createServiceAccount: true
    roleName: pod-identity-role-alb
    wellKnownPolicies:
      awsLoadBalancerController: true

  - namespace: default
    serviceAccountName: s3-reader
    createServiceAccount: true
    roleName: pod-identity-role-s3
    permissionPolicyARNs: ["arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"]

  - namespace: kube-system
    serviceAccountName: efs-csi-controller-sa
    createServiceAccount: true
    roleName: pod-identity-role-efs
    wellKnownPolicies:
      efsCSIController: true      

  - namespace: kube-system
    serviceAccountName: external-dns
    createServiceAccount: true
    roleName: pod-identity-role-external-dns
    wellKnownPolicies:
      externalDNS: true          

  - namespace: cert-manager
    serviceAccountName: cert-manager
    createServiceAccount: true
    roleName: pod-identity-role-cert-manager
    wellKnownPolicies:
      certManager: true         

  - namespace: kube-system
    serviceAccountName: cluster-autoscaler
    createServiceAccount: true
    roleName: pod-identity-role-cluster-autoscaler
    wellKnownPolicies:
      autoScaler: true  

  - namespace: amazon-cloudwatch
    serviceAccountName: aws-node
    createServiceAccount: true
    roleName: pod-identity-role-cloudwatch-agent
    permissionPolicyARNs: ["arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy"]

managedNodeGroups:
  - name: eks-poc-ng
    instanceType: t3a.medium 
    amiFamily: AmazonLinux2023
    privateNetworking: True # True Ensures nodes are launched in private subnets
   # updateConfig:
    #  maxUnavailablePercentage: 10
    disableIMDSv1: True
    disablePodIMDS: false
    desiredCapacity: 1
    minSize: 1
    maxSize: 2
    volumeSize: 20
    volumeType: gp3
#    volumeEncrypted: true
#    volumeKmsKeyID:  # please see https://docs.aws.amazon.com/autoscaling/ec2/userguide/key
    ssh: # use existing EC2 key
      allow: true
      #enableSsm: false # cannot disable SSM and built into AMI 
      publicKeyPath: "/root/.ssh/devops.pub"
    securityGroups:
      withShared: false #This disables the creation and attachment of the "shared" security group. 
      attachIDs: ["sg-06a0beefd0ed45a07"]
    maxPodsPerNode: 17
    subnets:
      - subnet-0ebeafd6054e83d84
      - subnet-0e62b2a66ea4235c7
  #    - subnet-0564dca4816439969	 
    iam:
      withAddonPolicies:
        imageBuilder: true
        autoScaler: true
        externalDNS: true
        certManager: true
        #appMesh: true
        ebs: true
        albIngress: true
        xRay: true
        cloudWatch: true
        awsLoadBalancerController: true
        efs: false
        fsx: false
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
        - arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
        #- arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess
        #- arn:aws:iam::1111111111:policy/kube2iam
      #attachPolicyARNs: https://dev.to/aws-builders/eks-iam-deep-dive-136d https://schema.eksctl.io/
        # - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
         #- arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
         #- arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
       # AWS Managed or Customer Managed IAM Policy ARN.
         #- arn:aws:iam::aws:policy/AmazonS3FullAccess
      #instanceProfileARN: "arn:aws:iam::509399617800:instance-profile/eksNodeRole"
      #instanceRoleARN: "arn:aws:iam::509399617800:role/eksNodeRole"
    tags:
      alpha.eksctl.io/cluster-name: "eks-poc"
      alpha.eksctl.io/nodegroup-name: "ops_build_eks_nodegroup"
      alpha.eksctl.io/nodegroup-type: "managed"
      nodegroup-role: "worker-node"
       # EC2 tags required for cluster-autoscaler auto-discovery - these tags are automatically applied to a managed nodegroup autoscaling group
      k8s.io/cluster-autoscaler/enabled: "true"
      k8s.io/cluster-autoscaler/managednodes-quickstart: "owned"  
    #privateCluster: # This option is required only when complete cluster is set to private
     #enabled: true  # instructs eksctl to not create the necessary AWS VPC endpoints (for services like ECR, S3, etc.) assuming that are already present in VPC)
     #skipEndpointCreation: true
    labels: 
      nodegroup-role: "worker" 

addons:
  #- name: aws-ebs-csi-driver
  #  version: latest
  - name: eks-pod-identity-agent
    version: latest
    resolveConflicts: overwrite
    useDefaultPodIdentityAssociations: true
  - name: vpc-cni
    version: latest # Example version
    resolveConflicts: overwrite
    useDefaultPodIdentityAssociations: true
    #attachPolicyARNs:
     #- arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
    # Optional: Add specific config for the CNI if needed
  - name: kube-proxy
    version: latest
    resolveConflicts: overwrite
    useDefaultPodIdentityAssociations: true
    # Optional: Configure settings here
  - name: coredns
    version: latest 
    resolveConflicts: overwrite
    useDefaultPodIdentityAssociations: true
  - name: aws-ebs-csi-driver # Example for a common add-on
    version: latest 
    resolveConflicts: overwrite
    wellKnownPolicies: # Simplifies IAM setup
      ebsCSIController: true
  
  
addonsConfig:  
# automatically resolve (and apply) the recommended pod identity configuration  
   autoApplyPodIdentityAssociations: true

karpenter:
  version: '0.37.0' # Exact version must be provided
  createServiceAccount: true # default is false
  withSpotInterruptionQueue: true #adds all required policies and rules for supporting Spot Interruption Queue, default is false