or create a new repository on the command line
echo "# eks_eksctl" >> README.md
git init
git add README.md
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/Mugila/eks_eksctl.git
git push -u origin main
â€¦or push an existing repository from the command line
git remote add origin https://github.com/Mugila/eks_eksctl.git
git branch -M main
git push -u origin main

### uninstall failed  helm release 

root@ip-10-0-7-9:~/eks_eksctl# helm list --all-namespaces 
NAME        	NAMESPACE  	REVISION	UPDATED                                	STATUS  	CHART              	APP VERSION
external-dns	kube-system	1       	2025-12-11 22:31:36.715507669 +0000 UTC	deployed	external-dns-1.19.0	0.19.0     
karpenter   	karpenter  	1       	2025-12-11 16:26:08.801163341 +0000 UTC	deployed	karpenter-0.37.0   	0.37.0  

root@ip-10-0-7-9:~/eks_eksctl# helm list --all-namespaces
NAME               	NAMESPACE  	REVISION	UPDATED                                	STATUS  	CHART              	APP VERSION
external-dns       	kube-system	1       	2025-12-11 22:31:36.715507669 +0000 UTC	deployed	external-dns-1.19.0	0.19.0     
externaldns-release	kube-system	3       	2025-12-11 21:56:37.947138986 +0000 UTC	failed  	external-dns-9.0.3 	0.18.0     
karpenter          	karpenter  	1       	2025-12-11 16:26:08.801163341 +0000 UTC	deployed	karpenter-0.37.0   	0.37.0     
root@ip-10-0-7-9:~/eks_eksctl# helm get values external-dns -n kube-system 
USER-SUPPLIED VALUES:
aws:
  zoneType: public
domainFilters:
- aadhavan.us
policy: sync
provider:
  name: aws
serviceAccount:
  create: false
  name: external-dns
txtOwnerId: Z04559112EZU5BH7HHOSM
root@ip-10-0-7-9:~/eks_eksctl# helm get values karpenter  -n karpenter
USER-SUPPLIED VALUES:
aws:
  defaultInstanceProfile: eksctl-KarpenterNodeInstanceProfile-eks-poc
clusterEndpoint: https://309D9AF433BE36CBC807C75B50152EC2.gr7.us-east-1.eks.amazonaws.com
clusterName: eks-poc
serviceAccount:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::509399617800:role/eksctl-eks-poc-iamservice-role
  create: true
  name: karpenter
settings:
  clusterEndpoint: https://309D9AF433BE36CBC807C75B50152EC2.gr7.us-east-1.eks.amazonaws.com
  clusterName: eks-poc
  defaultInstanceProfile: eksctl-KarpenterNodeInstanceProfile-eks-poc
  interruptionQueueName: eks-poc


root@ip-10-0-7-9:~/eks_eksctl# helm uninstall externaldns-release  --namespace kube-system
release "externaldns-release" uninstalled
root@ip-10-0-7-9:~/eks_eksctl# helm list 
NAME	NAMESPACE	REVISION	UPDATED	STATUS	CHART	APP VERSION
root@ip-10-0-7-9:~/eks_eksctl# helm list --all-namespaces 
NAME        	NAMESPACE  	REVISION	UPDATED                                	STATUS  	CHART              	APP VERSION
external-dns	kube-system	1       	2025-12-11 22:31:36.715507669 +0000 UTC	deployed	external-dns-1.19.0	0.19.0     
karpenter   	karpenter  	1       	2025-12-11 16:26:08.801163341 +0000 UTC	deployed	karpenter-0.37.0   	0.37.0     
root@ip-10-0-7-9:~/eks_eksctl# 



echo "The EKS cluster name is: $CLUSTER_NAME"

# Example: Update your kubeconfig file using the variable
aws eks update-kubeconfig --name "$CLUSTER_NAME" --region <YOUR_AWS_REGION>

# Example: Describe the cluster's OIDC issuer
aws eks describe-cluster --name "$CLUSTER_NAME" --query "cluster.identity.oidc.issuer" --output text

https://medium.com/@mb20261/aws-cli-v2-by-examples-mastering-amazon-eks-management-with-kubectl-integration-a6096895ebfc



 eksctl create iamserviceaccount -f  cluster_creation.yaml  --approve 
eksctl create  cluster -f cluster_creation.yaml  --timeout=45m  --verbose 4


create only service account from existing config file 
eksctl delete iamserviceaccount --cluster=eks-poc  --name=cert-manager --namespace=kube-system
eksctl create iamserviceaccount -f  cluster_creation.yaml  --approve 
eksctl delete   cluster -f cluster_creation.yaml 
eksctl create  cluster -f cluster_creation.yaml  --timeout=45m  --verbose 4


Recreate serviceaccount cert-manager  with name space cert-manager 
eksctl delete iamserviceaccount --cluster=eks-poc --name=cert-manager --namespace=cert-manager
eksctl create iamserviceaccount -f  cluster_creation.yaml  --approve



full clean up of certmanager  (deleted namespace cert-manager installed through eksctl cluster config )
https://cert-manager.io/docs/installation/kubectl/#uninstalling



helm list -n cert-manager
helm uninstall cert-manager  --namespace cert-manager  --dry-run
kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.19.2/cert-manager.yaml
kubectl delete lease -n cert-manager  cert-manager-cainjector-leader-election cert-manager-controller


You can skip the failing post-install hook to get cert-manager installed, which makes further debugging easier.
helm install cert-manager jetstack/cert-manager   --namespace cert-manager   --version v1.19.2   --set installCRDs=true   --set serviceAccount.create=false   --set serviceAccount.name=default   --set automountServiceAccountToken=true --set startupapicheck.timeout=5m  --no-hooks
kubect get all -n cert-manager 

helm install cert-manager jetstack/cert-manager   --namespace cert-manager   --version v1.19.2   --set installCRDs=true   --set serviceAccount.create=false   --set serviceAccount.name=default   --set automountServiceAccountToken=true --set startupapicheck.timeout=5m  --set webhook.timeoutSeconds=30


kubectl get all -n cert-manager   get all resources running 

root@ip-10-0-7-9:~/eks_eksctl# kubectl logs --previous pod/cert-manager-startupapicheck-7qcz9  -n cert-manager
root@ip-10-0-7-9:~/eks_eksctl# kubectl logs pod/cert-manager-startupapicheck-7qcz9  -n cert-manager

check events 
kubectl get events --namespace cert-manager
kubectl describe pods cert-manager-startupapicheck-7qcz9 -n cert-managerf