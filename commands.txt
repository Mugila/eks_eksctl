or create a new repository on the command line
echo "# eks_eksctl" >> README.md
git init
git add README.md
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/Mugila/eks_eksctl.git
git push -u origin main
â€¦or push an existing repository from the command line
git remote add origin https://github.com/Mugila/eks_eksctl.git
git branch -M main
git push -u origin main

### uninstall failed  helm release 

root@ip-10-0-7-9:~/eks_eksctl# helm list --all-namespaces 
NAME        	NAMESPACE  	REVISION	UPDATED                                	STATUS  	CHART              	APP VERSION
external-dns	kube-system	1       	2025-12-11 22:31:36.715507669 +0000 UTC	deployed	external-dns-1.19.0	0.19.0     
karpenter   	karpenter  	1       	2025-12-11 16:26:08.801163341 +0000 UTC	deployed	karpenter-0.37.0   	0.37.0  

root@ip-10-0-7-9:~/eks_eksctl# helm list --all-namespaces
NAME               	NAMESPACE  	REVISION	UPDATED                                	STATUS  	CHART              	APP VERSION
external-dns       	kube-system	1       	2025-12-11 22:31:36.715507669 +0000 UTC	deployed	external-dns-1.19.0	0.19.0     
externaldns-release	kube-system	3       	2025-12-11 21:56:37.947138986 +0000 UTC	failed  	external-dns-9.0.3 	0.18.0     
karpenter          	karpenter  	1       	2025-12-11 16:26:08.801163341 +0000 UTC	deployed	karpenter-0.37.0   	0.37.0     
root@ip-10-0-7-9:~/eks_eksctl# helm get values external-dns -n kube-system 
USER-SUPPLIED VALUES:
aws:
  zoneType: public
domainFilters:
- aadhavan.us
policy: sync
provider:
  name: aws
serviceAccount:
  create: false
  name: external-dns
txtOwnerId: Z04559112EZU5BH7HHOSM
root@ip-10-0-7-9:~/eks_eksctl# helm get values karpenter  -n karpenter
USER-SUPPLIED VALUES:
aws:
  defaultInstanceProfile: eksctl-KarpenterNodeInstanceProfile-eks-poc
clusterEndpoint: https://309D9AF433BE36CBC807C75B50152EC2.gr7.us-east-1.eks.amazonaws.com
clusterName: eks-poc
serviceAccount:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::509399617800:role/eksctl-eks-poc-iamservice-role
  create: true
  name: karpenter
settings:
  clusterEndpoint: https://309D9AF433BE36CBC807C75B50152EC2.gr7.us-east-1.eks.amazonaws.com
  clusterName: eks-poc
  defaultInstanceProfile: eksctl-KarpenterNodeInstanceProfile-eks-poc
  interruptionQueueName: eks-poc


root@ip-10-0-7-9:~/eks_eksctl# helm uninstall externaldns-release  --namespace kube-system
release "externaldns-release" uninstalled
root@ip-10-0-7-9:~/eks_eksctl# helm list 
NAME	NAMESPACE	REVISION	UPDATED	STATUS	CHART	APP VERSION
root@ip-10-0-7-9:~/eks_eksctl# helm list --all-namespaces 
NAME        	NAMESPACE  	REVISION	UPDATED                                	STATUS  	CHART              	APP VERSION
external-dns	kube-system	1       	2025-12-11 22:31:36.715507669 +0000 UTC	deployed	external-dns-1.19.0	0.19.0     
karpenter   	karpenter  	1       	2025-12-11 16:26:08.801163341 +0000 UTC	deployed	karpenter-0.37.0   	0.37.0     
root@ip-10-0-7-9:~/eks_eksctl# 



echo "The EKS cluster name is: $CLUSTER_NAME"

# Example: Update your kubeconfig file using the variable
aws eks update-kubeconfig --name "$CLUSTER_NAME" --region <YOUR_AWS_REGION>

# Example: Describe the cluster's OIDC issuer
aws eks describe-cluster --name "$CLUSTER_NAME" --query "cluster.identity.oidc.issuer" --output text

https://medium.com/@mb20261/aws-cli-v2-by-examples-mastering-amazon-eks-management-with-kubectl-integration-a6096895ebfc



 eksctl create iamserviceaccount -f  cluster_creation.yaml  --approve 
eksctl create  cluster -f cluster_creation.yaml  --timeout=45m  --verbose 4


create only service account from existing config file 
eksctl delete iamserviceaccount --cluster=eks-poc  --name=cert-manager --namespace=kube-system
eksctl create iamserviceaccount -f  cluster_creation.yaml  --approve 
eksctl delete   cluster -f cluster_creation.yaml 
eksctl create  cluster -f cluster_creation.yaml  --timeout=45m  --verbose 4


Recreate serviceaccount cert-manager  with name space cert-manager 
eksctl delete iamserviceaccount --cluster=eks-poc --name=cert-manager --namespace=cert-manager
eksctl create iamserviceaccount -f  cluster_creation.yaml  --approve



full clean up of certmanager  (deleted namespace cert-manager installed through eksctl cluster config )
https://cert-manager.io/docs/installation/kubectl/#uninstalling



helm list -n cert-manager
helm uninstall cert-manager  --namespace cert-manager  --dry-run
kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.19.2/cert-manager.yaml
kubectl delete lease -n cert-manager  cert-manager-cainjector-leader-election cert-manager-controller


You can skip the failing post-install hook to get cert-manager installed, which makes further debugging easier.
helm install cert-manager jetstack/cert-manager   --namespace cert-manager   --version v1.19.2   --set installCRDs=true   --set serviceAccount.create=false   --set serviceAccount.name=default   --set automountServiceAccountToken=true --set startupapicheck.timeout=5m  --no-hooks
kubect get all -n cert-manager 

helm install cert-manager jetstack/cert-manager   --namespace cert-manager     --set installCRDs=true   --set serviceAccount.create=false   --set serviceAccount.name=default   --set automountServiceAccountToken=true --set startupapicheck.timeout=5m  --set webhook.timeoutSeconds=30


kubectl get all -n cert-manager   get all resources running 

root@ip-10-0-7-9:~/eks_eksctl# kubectl logs --previous pod/cert-manager-startupapicheck-7qcz9  -n cert-manager
root@ip-10-0-7-9:~/eks_eksctl# kubectl logs pod/cert-manager-startupapicheck-7qcz9  -n cert-manager

check events 
kubectl get events --namespace cert-manager
kubectl describe pods cert-manager-startupapicheck-7qcz9 -n cert-managerf

kubectl -n cert-manager  logs  -l app.kubernetes.io/name=cert-manager --follow 
kubectl -n cert-manager  logs  -l app.kubernetes.io/name=cert-manager --tail=2


cretae new node group
eksctl create nodegroup --config-file=cluster_creation.yaml --include=eks-poc-ng2 
eksctl get nodegroups --cluster=eks-poc

delete nodegroup
eksctl delete nodegroup --cluster=eks-poc --name=eks-poc-ng2
root@ip-10-0-7-9:~/eks_eksctl# kubectl get nodes -o wide 



How to check number of pods running in each node 
kubectl get pods --all-namespaces -o jsonpath='{range .items[?(@.spec.nodeName)]}{.spec.nodeName}{"\n"}{end}' | sort | uniq -c | sort -rn | awk 'BEGIN {print "Count\tNodeName"} {print $1 "\t" $2}'
Count	NodeName
17	ip-10-0-8-40.ec2.internal
7	ip-10-0-4-98.ec2.internal
4	ip-10-0-8-125.ec2.internal
root@ip-10-0-7-9:~/eks_eksctl# 



kubectl -n app1 get all
root@ip-10-0-7-9:~/eks_eksctl# kubectl scale deployment.apps/app1 --replicas=0 -n app1


root@ip-10-0-7-9:~/eks_eksctl# kubectl scale deployment.apps/app1 --replicas=1 -n app1
deployment.apps/app1 scaled
root@ip-10-0-7-9:~/eks_eksctl# kubectl get pods -n app1
NAME                    READY   STATUS    RESTARTS   AGE


### Drain and Migrate Workloads

root@ip-10-0-7-9:~/eks_eksctl# kubectl get pods --all-namespaces -o jsonpath='{range .items[?(@.spec.nodeName)]}{.spec.nodeName}{"\n"}{end}' | sort | uniq -c | sort -rn | awk 'BEGIN {print "Count\tNodeName"} {print $1 "\t" $2}'
Count	NodeName
17	ip-10-0-8-40.ec2.internal
7	ip-10-0-8-75.ec2.internal

root@ip-10-0-7-9:~/eks_eksctl# kubectl get nodes -o wide 
NAME                        STATUS   ROLES    AGE     VERSION                INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                       KERNEL-VERSION                    CONTAINER-RUNTIME
ip-10-0-8-40.ec2.internal   Ready    <none>   6h17m   v1.30.14-eks-ecaa3a6   10.0.8.40     <none>        Amazon Linux 2023.9.20251208   6.1.158-180.294.amzn2023.x86_64   containerd://2.1.5
ip-10-0-8-75.ec2.internal   Ready    <none>   2m55s   v1.30.14-eks-ecaa3a6   10.0.8.75     <none>        Amazon Linux 2023.9.20251208   6.1.158-180.294.amzn2023.x86_64   containerd://2.1.5


Monitor the number pod on each node during the process
kubectl cordon ip-10-0-8-40.ec2.internal
kubectl drain ip-10-0-8-40.ec2.internal  --ignore-daemonsets --delete-emptydir-data

root@ip-10-0-7-9:~/eks_eksctl# kubectl get nodes -w
NAME                        STATUS                     ROLES    AGE     VERSION
ip-10-0-8-40.ec2.internal   Ready,SchedulingDisabled   <none>   6h24m   v1.30.14-eks-ecaa3a6
ip-10-0-8-75.ec2.internal   Ready                      <none>   9m45s   v1.30.14-eks-ecaa3a6
^Croot@ip-10-0-7-9:~/eks_eksctl# kubectl get pods --all-namespaces -o jsonpath='{range .items[?(@.spec.nodeName)]}{.spec.nodeName}{"\n"}{end}' | sort | uniq -c | sort -rn | awk 'BEGIN {print "Count\tNodeName"} {print $1 "\t" $2}'
Count	NodeName
17	ip-10-0-8-75.ec2.internal
4	ip-10-0-8-40.ec2.internal
root@ip-10-0-7-9:~/eks_eksctl# 
root@ip-10-0-7-9:~/eks_eksctl# 

Delete the node 
eksctl delete nodegroup --cluster=eks-poc --name=eks-poc-ng


root@ip-10-0-7-9:~/eks_eksctl# kubectl get pods --all-namespaces -o jsonpath='{range .items[?(@.spec.nodeName)]}{.spec.nodeName}{"\n"}{end}' | sort | uniq -c | sort -rn | awk 'BEGIN {print "Count\tNodeName"} {print $1 "\t" $2}'
Count	NodeName
17	ip-10-0-8-75.ec2.internal
root@ip-10-0-7-9:~/eks_eksctl# kubectl  get pods -A
NAMESPACE      NAME                                            READY   STATUS    RESTARTS   AGE
app1           app1-7dddd5646f-fznrs                           1/1     Running   0          31m
app2           app2-7bc64754f9-xmc62                           1/1     Running   0          31m
cert-manager   cert-manager-cainjector-84df87b75c-295q4        1/1     Running   0          13m
cert-manager   cert-manager-fbd88c775-47j4d                    1/1     Running   0          13m
cert-manager   cert-manager-webhook-95dbdbb-psbfd              0/1     Pending   0          12m
karpenter      karpenter-7b9469d845-fxw2f                      0/1     Pending   0          13m
karpenter      karpenter-7b9469d845-svqkl                      1/1     Running   0          31m
kube-system    aws-load-balancer-controller-74f87ff7b4-ld4wc   1/1     Running   0          13m
kube-system    aws-load-balancer-controller-74f87ff7b4-p4mcj   1/1     Running   0          13m
kube-system    aws-node-bqzmr                                  2/2     Running   0          18m
kube-system    coredns-9686cdd4d-wkkhn                         1/1     Running   0          12m
kube-system    coredns-9686cdd4d-wwptd                         1/1     Running   0          13m
kube-system    ebs-csi-controller-59cfdcf6c8-fv6hv             6/6     Running   0          13m
kube-system    ebs-csi-controller-59cfdcf6c8-vgpnn             6/6     Running   0          12m
kube-system    ebs-csi-node-t4jgk                              3/3     Running   0          18m
kube-system    eks-pod-identity-agent-zkwcv                    1/1     Running   0          18m
kube-system    external-dns-5cd67f9577-62pxf                   0/1     Pending   0          12m
kube-system    kube-proxy-5q87w                                1/1     Running   0          18m
kube-system    metrics-server-59c6c74c8d-htfhj                 1/1     Running   0          13m
kube-system    metrics-server-59c6c74c8d-znp6d                 1/1     Running   0          12m


Create a new node with t3 small 

root@ip-10-0-7-9:~/eks_eksctl# eksctl create nodegroup --config-file=cluster_creation.yaml --include=eks-poc-ng
root@ip-10-0-7-9:~/eks_eksctl# kubectl get pods -A -o=custom-columns=NAMESPACE:.metadata.namespace,POD:.metadata.name,NODE:.spec.nodeName

root@ip-10-0-7-9:~/eks_eksctl# kubectl get pods -A -o wide
NAMESPACE      NAME                                            READY   STATUS    RESTARTS   AGE   IP           NODE                        NOMINATED NODE   READINESS GATES
app1           app1-7dddd5646f-fznrs                           1/1     Running   0          78m   10.0.8.7     ip-10-0-8-75.ec2.internal   <none>           <none>
app2           app2-7bc64754f9-xmc62                           1/1     Running   0          78m   10.0.8.52    ip-10-0-8-75.ec2.internal   <none>           <none>
cert-manager   cert-manager-cainjector-84df87b75c-295q4        1/1     Running   0          59m   10.0.8.120   ip-10-0-8-75.ec2.internal   <none>           <none>
cert-manager   cert-manager-fbd88c775-47j4d                    1/1     Running   0          59m   10.0.8.74    ip-10-0-8-75.ec2.internal   <none>           <none>
cert-manager   cert-manager-webhook-95dbdbb-psbfd              1/1     Running   0          59m   10.0.8.21    ip-10-0-8-69.ec2.internal   <none>           <none>
karpenter      karpenter-7b9469d845-fxw2f                      1/1     Running   0          59m   10.0.8.37    ip-10-0-8-69.ec2.internal   <none>           <none>
karpenter      karpenter-7b9469d845-svqkl                      1/1     Running   0          78m   10.0.8.11    ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    aws-load-balancer-controller-74f87ff7b4-ld4wc   1/1     Running   0          59m   10.0.8.61    ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    aws-load-balancer-controller-74f87ff7b4-p4mcj   1/1     Running   0          59m   10.0.8.108   ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    aws-node-2dwm9                                  2/2     Running   0          40m   10.0.8.69    ip-10-0-8-69.ec2.internal   <none>           <none>
kube-system    aws-node-bqzmr                                  2/2     Running   0          65m   10.0.8.75    ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    coredns-9686cdd4d-wkkhn                         1/1     Running   0          59m   10.0.8.102   ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    coredns-9686cdd4d-wwptd                         1/1     Running   0          59m   10.0.8.27    ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    ebs-csi-controller-59cfdcf6c8-fv6hv             6/6     Running   0          59m   10.0.8.72    ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    ebs-csi-controller-59cfdcf6c8-vgpnn             6/6     Running   0          59m   10.0.8.62    ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    ebs-csi-node-t4jgk                              3/3     Running   0          65m   10.0.8.66    ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    ebs-csi-node-x7vdz                              3/3     Running   0          40m   10.0.8.105   ip-10-0-8-69.ec2.internal   <none>           <none>
kube-system    eks-pod-identity-agent-cjnpj                    1/1     Running   0          40m   10.0.8.69    ip-10-0-8-69.ec2.internal   <none>           <none>
kube-system    eks-pod-identity-agent-zkwcv                    1/1     Running   0          65m   10.0.8.75    ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    external-dns-5cd67f9577-62pxf                   1/1     Running   0          59m   10.0.8.54    ip-10-0-8-69.ec2.internal   <none>           <none>
kube-system    kube-proxy-5q87w                                1/1     Running   0          65m   10.0.8.75    ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    kube-proxy-6pjpx                                1/1     Running   0          40m   10.0.8.69    ip-10-0-8-69.ec2.internal   <none>           <none>
kube-system    metrics-server-59c6c74c8d-htfhj                 1/1     Running   0          59m   10.0.8.23    ip-10-0-8-75.ec2.internal   <none>           <none>
kube-system    metrics-server-59c6c74c8d-znp6d                 1/1     Running   0          59m   10.0.8.60    ip-10-0-8-75.ec2.internal   <none>           <none>

root@ip-10-0-7-9:~/eks_eksctl# kubectl get pods --all-namespaces --field-selector spec.nodeName=ip-10-0-8-69.ec2.internal
NAMESPACE      NAME                                 READY   STATUS    RESTARTS   AGE
cert-manager   cert-manager-webhook-95dbdbb-psbfd   1/1     Running   0          62m
karpenter      karpenter-7b9469d845-fxw2f           1/1     Running   0          62m
kube-system    aws-node-2dwm9                       2/2     Running   0          43m
kube-system    ebs-csi-node-x7vdz                   3/3     Running   0          43m
kube-system    eks-pod-identity-agent-cjnpj         1/1     Running   0          43m
kube-system    external-dns-5cd67f9577-62pxf        1/1     Running   0          62m
kube-system    kube-proxy-6pjpx                     1/1     Running   0          43m
root@ip-10-0-7-9:~/eks_eksctl# kubectl get pods --all-namespaces --field-selector spec.nodeName=ip-10-0-8-75.ec2.internal
NAMESPACE      NAME                                            READY   STATUS    RESTARTS   AGE
app1           app1-7dddd5646f-fznrs                           1/1     Running   0          81m
app2           app2-7bc64754f9-xmc62                           1/1     Running   0          81m
cert-manager   cert-manager-cainjector-84df87b75c-295q4        1/1     Running   0          63m
cert-manager   cert-manager-fbd88c775-47j4d                    1/1     Running   0          63m
karpenter      karpenter-7b9469d845-svqkl                      1/1     Running   0          81m
kube-system    aws-load-balancer-controller-74f87ff7b4-ld4wc   1/1     Running   0          63m
kube-system    aws-load-balancer-controller-74f87ff7b4-p4mcj   1/1     Running   0          63m
kube-system    aws-node-bqzmr                                  2/2     Running   0          68m
kube-system    coredns-9686cdd4d-wkkhn                         1/1     Running   0          63m
kube-system    coredns-9686cdd4d-wwptd                         1/1     Running   0          63m
kube-system    ebs-csi-controller-59cfdcf6c8-fv6hv             6/6     Running   0          63m
kube-system    ebs-csi-controller-59cfdcf6c8-vgpnn             6/6     Running   0          62m
kube-system    ebs-csi-node-t4jgk                              3/3     Running   0          68m
kube-system    eks-pod-identity-agent-zkwcv                    1/1     Running   0          68m
kube-system    kube-proxy-5q87w                                1/1     Running   0          68m
kube-system    metrics-server-59c6c74c8d-htfhj                 1/1     Running   0          63m
kube-system    metrics-server-59c6c74c8d-znp6d                 1/1     Running   0          62m


kubectl get nodes -o wide --label-columns topology.kubernetes.io/zone



####### How to install ArgoCD 
root@ip-10-0-7-9:~/eks_eksctl# helm list --all-namespaces
NAME                        	NAMESPACE   	REVISION	UPDATED                                	STATUS  	CHART                              	APP VERSION
aws-load-balancer-controller	kube-system 	1       	2025-12-16 13:20:00.710607102 +0000 UTC	deployed	aws-load-balancer-controller-1.16.0	v2.16.0    
cert-manager                	cert-manager	1       	2025-12-16 13:56:52.171428906 +0000 UTC	deployed	cert-manager-v1.19.2               	v1.19.2    
external-dns                	kube-system 	1       	2025-12-16 13:21:19.253597494 +0000 UTC	deployed	external-dns-1.19.0                	0.19.0     
karpenter                   	karpenter   	1       	2025-12-16 12:55:38.000057688 +0000 UTC	deployed	karpenter-0.37.0                   	0.37.0     
my-argo-cd                  	argocd      	1       	2025-12-17 02:22:12.975173032 +0000 UTC	failed  	argo-cd-4.8.0                      	v2.3.4     
root@ip-10-0-7-9:~/eks_eksctl# helm uninstall my-argo-cd  --namespace argocd 
release "my-argo-cd" uninstalled
root@ip-10-0-7-9:~/eks_eksctl# kubectl get all  -n argocd 
No resources found in argocd namespace. 



### Exporting helm values from helm chart 
helm show values argo/argo-cd > values.yaml

helm repo add argo https://argoproj.github.io/argo-helm
helm repo update
helm repo add argo https://argoproj.github.io/argo-helm

helm install argocd argo/argo-cd --namespace argocd  --version 4.8.0 --values argo-non-ha.yaml --wait 

